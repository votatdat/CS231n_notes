{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. Accuracy\n",
    "2. Recall\n",
    "3. Precision\n",
    "4. F1 Score\n",
    "5. Review\n",
    "\n",
    "These contents are from https://www.codecademy.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a machine learning algorithm capable of making classifications, the next step in the process is to calculate its predictive power. In order to calculate these statistics, we'll need to split our data into a training set and validation set.\n",
    "\n",
    "Let's say you're using a machine learning algorithm to try to predict whether or not you will get above a B on a test. The features of your data could be something like:\n",
    "- The number of hours you studied this week.\n",
    "- The number of hours you watched Netflix this week.\n",
    "- The time you went to bed the night before the test.\n",
    "- Your average in the class before taking the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way of reporting the effectiveness of an algorithm is by calculating its **`accuracy`**. Accuracy is calculated by finding the total number of correctly classified points and dividing by the total number of points.\n",
    "\n",
    "In other words, accuracy can be defined as:\n",
    "\n",
    "$$\\frac{(True\\ Positives + True\\ Negatives)}{(True\\ Positives + True\\ Negatives + False\\ Positives + False\\ Negatives)}$$\n",
    "- True Positive: The algorithm predicted you would get above a B, and you did.\n",
    "- True Negative: The algorithm predicted you would get below a B, and you did.\n",
    "- False Positive: The algorithm predicted you would get above a B, and you didn’t.\n",
    "- False Negative: The algorithm predicted you would get below a B, and you didn’t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3000\n"
     ]
    }
   ],
   "source": [
    "labels =  [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == guesses[i] == 1:\n",
    "        true_positives += 1\n",
    "    if labels[i] == guesses[i] == 0:\n",
    "        true_negatives += 1\n",
    "    if labels[i] != guesses[i] == 1:\n",
    "        false_positives += 1\n",
    "    if labels[i] != guesses[i] == 0:\n",
    "        false_negatives += 1\n",
    "    \n",
    "accuracy = (true_positives + true_negatives)/len(labels)\n",
    "print('Accuracy: %.4f' %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recall\n",
    "**Accuracy** can be an extremely misleading statistic depending on your data. Consider the example of an algorithm that is trying to predict whether or not there will be over 3 feet of snow on the ground tomorrow. We can write a pretty accurate classifier right now: always predict False. This classifier will be incredibly accurate — there are hardly ever many days with that much snow. But this classifier never finds the information we’re actually interested in.\n",
    "\n",
    "In this situation, the statistic that would be helpful is **`recall`**. Recall measures the percentage of relevant items that your classifier found. In this example, recall is the number of snow days the algorithm correctly predicted divided by the total number of snow days. Another way of saying this is:\n",
    "\n",
    "$$\\frac{True\\ Positives}{(True\\ Positives + False\\ Negatives)}$$\n",
    "\n",
    "Our algorithm that *always* predicts False might have a very high accuracy, but it *never* will find any True Positives, so its `recall` is `0`. This makes sense; recall should be very low for such an absurd classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4286\n"
     ]
    }
   ],
   "source": [
    "recall = true_positives/(true_positives + false_negatives)\n",
    "print('Recall: %.4f' %recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precision\n",
    "Unfortunately, `recall` isn't a perfect statistic either. For example, we could create a snow day classifier that always returns True. This would have low accuracy, but its recall would be 1 because it would be able to accurately find every snow day. But this classifier is just as nonsensical as the one before! The statistic that will help demonstrate that this algorithm is flawed is precision.\n",
    "\n",
    "In the snow day example, **`precision`** is the number of snow days the algorithm correctly predicted divided by the number of times it predicted there would be a snow day. The formula for precision is below:\n",
    "\n",
    "$$\\frac{True\\ Positives}{(True\\ Positives + False\\ Positives)}$$\n",
    "\n",
    "The algorithm that predicts every day is a snow day has recall of 1, but it will have very low precision. It correctly predicts every snow day, but there are tons of false positives as well.\n",
    "\n",
    "Precision and recall are statistics that are on opposite ends of a scale. If one goes down, the other will go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5000\n"
     ]
    }
   ],
   "source": [
    "precision = true_positives / (true_positives + false_positives)\n",
    "print('Precision: %.4f' %precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. F1 Score\n",
    "It is useful to consider the `precision` and `recall` of an algorithm, however, we still don't have one number that can sufficiently describe how effective our algorithm is. This is the job of the **`F1 score`** — **F1 score is the harmonic mean of precision and recall**. The harmonic mean of a group of numbers is a way to average them together. The formula for F1 score is below:\n",
    "$$F1 = 2 * \\frac{(precision * recall)}{(precision + recall)}$$\n",
    "The F1 score combines both precision and recall into a single statistic. We use the harmonic mean rather than the traditional arithmetic mean because we want the F1 score to have a low value when either precision or recall is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider a classifier where `recall` = 1 and `precision` = 0.01. We know that there is most likely a problem with this classifier since the `precision` is so low, and so we want the F1 score to reflect that.\n",
    "\n",
    "If we took the arithmetic mean, we’d get:\n",
    "$$\\frac{(1 + 0.01)}{2} = 0.505$$\n",
    "That looks way too high! But if we calculate the harmonic mean, we get:\n",
    "$$2 * \\frac{(1 * 0.01)}{(1 + 0.01)} = 0.019$$\n",
    "That’s much better! The F1 score is now accurately describing the effectiveness of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.4615\n"
     ]
    }
   ],
   "source": [
    "f_1 = 2*(recall*precision)/(recall+precision)\n",
    "print('F1 score: %.4f' %f_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Review\n",
    "You’ve now learned many different ways to analyze the predictive power of your algorithm. Some of the key insights for this course include:\n",
    "- Classifying a single point can result in: \n",
    "    - a true positive (truth = 1, guess = 1), \n",
    "    - a true negative (truth = 0, guess = 0), \n",
    "    - a false positive (truth = 0, guess = 1), \n",
    "    - a false negative (truth = 1, guess = 0).\n",
    "- `Accuracy` measures how many classifications your algorithm got correct out of every classification it made.\n",
    "- `Recall` measures the percentage of the relevant items your classifier was able to successfully find.\n",
    "- `Precision` measures the percentage of items your classifier found that were actually relevant.\n",
    "- Precision and recall are tied to each other. As one goes up, the other will go down.\n",
    "- `F1 score` is a combination of precision and recall.\n",
    "- F1 score will be low if either precision or recall is low.\n",
    "\n",
    "The decision to use precision, recall, or F1 score ultimately comes down to the context of your classification. Maybe you don't care if your classifier has a lot of false positives. If that's the case, precision doesn't matter as much.\n",
    "\n",
    "As long as you have an understanding of what question you're trying to answer, you should be able to determine which statistic is most relevant to you.\n",
    "\n",
    "The Python library scikit-learn has some functions that will calculate these statistics for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3\n",
      "Recall: 0.42857142857142855\n",
      "Precision: 0.5\n",
      "F1 score: 0.4615384615384615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "labels =  [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print('Accuracy:', accuracy_score(labels, guesses))\n",
    "print('Recall:', recall_score(labels, guesses))\n",
    "print('Precision:', precision_score(labels, guesses))\n",
    "print('F1 score:', f1_score(labels, guesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
